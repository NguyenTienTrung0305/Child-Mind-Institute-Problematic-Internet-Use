{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":226.817896,"end_time":"2024-12-19T21:32:06.468565","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-19T21:28:19.650669","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import clone\n\nimport optuna\nfrom scipy.optimize import minimize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import *\nfrom scipy.stats import linregress\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\n\nimport re\n\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n\n\nSEED = 42\nn_splits = 5\nn_repeats = 3","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-24T10:32:23.193246Z","iopub.execute_input":"2024-12-24T10:32:23.193584Z","iopub.status.idle":"2024-12-24T10:32:26.551641Z","shell.execute_reply.started":"2024-12-24T10:32:23.193522Z","shell.execute_reply":"2024-12-24T10:32:26.550470Z"},"papermill":{"duration":19.799963,"end_time":"2024-12-19T21:28:41.875979","exception":false,"start_time":"2024-12-19T21:28:22.076016","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"## Process file\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().loc[['max', 'min', 'mean', '25%', '75%', 'std']].values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"Stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:32:26.554035Z","iopub.execute_input":"2024-12-24T10:32:26.554531Z","iopub.status.idle":"2024-12-24T10:32:26.562068Z","shell.execute_reply.started":"2024-12-24T10:32:26.554496Z","shell.execute_reply":"2024-12-24T10:32:26.560947Z"},"papermill":{"duration":0.014289,"end_time":"2024-12-19T21:28:42.15187","exception":false,"start_time":"2024-12-19T21:28:42.137581","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntrain_ts\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:32:26.563336Z","iopub.execute_input":"2024-12-24T10:32:26.563738Z","iopub.status.idle":"2024-12-24T10:33:46.719485Z","shell.execute_reply.started":"2024-12-24T10:32:26.563694Z","shell.execute_reply":"2024-12-24T10:33:46.718419Z"},"papermill":{"duration":165.154888,"end_time":"2024-12-19T21:31:27.320036","exception":false,"start_time":"2024-12-19T21:28:42.165148","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [01:19<00:00, 12.50it/s]\n100%|██████████| 2/2 [00:00<00:00,  9.15it/s]\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       Stat_0    Stat_1    Stat_2    Stat_3     Stat_4  Stat_5       Stat_6  \\\n0    1.850391  3.580182  1.738203  5.314874  89.422226     0.0  2626.199951   \n1    1.928769  3.234613  2.475326  3.966906  89.080330     1.0  2628.199951   \n2    1.021510  1.016589  1.746797  5.066334  86.987267     0.0  2618.199951   \n3    5.908000  2.083693  1.269051  6.134459  89.976074     0.0  2502.000000   \n4    3.231563  1.033620  1.071875  2.774382  89.300034     0.0  1046.800049   \n..        ...       ...       ...       ...        ...     ...          ...   \n991  1.491908  3.059684  2.099614  3.669502  89.025551     1.0  2576.399902   \n992  1.353594  0.994583  0.996484  1.786410  81.665283     0.0  1526.599976   \n993  0.999923  1.043029  1.547813  3.692727  89.333710     1.0  2592.199951   \n994  1.004674  0.981576  0.999219  1.673958  88.629547     0.0  1875.199951   \n995  1.015231  1.051578  1.006835  1.009104  88.652969     1.0  1196.599976   \n\n     Stat_7        Stat_8  Stat_9  Stat_10  Stat_11   Stat_12   Stat_13  \\\n0    4187.0  8.639500e+13     7.0      2.0     57.0 -1.812031 -2.631380   \n1    4146.0  8.639500e+13     7.0      2.0    243.0 -1.807955 -2.887664   \n2    4183.0  8.636500e+13     7.0      3.0    134.0 -1.903281 -3.150104   \n3    6000.0  8.639500e+13     7.0      4.0     72.0 -1.684624 -2.405738   \n4    4199.0  8.601500e+13     7.0      4.0     76.0 -1.675859 -1.071042   \n..      ...           ...     ...      ...      ...       ...       ...   \n991  4191.0  8.639500e+13     7.0      4.0    161.0 -1.407426 -1.014350   \n992  4194.0  8.514000e+13     7.0      2.0    130.0 -1.064844 -1.012995   \n993  4178.0  8.639500e+13     7.0      1.0     79.0 -1.508058 -2.958281   \n994  4183.0  8.639500e+13     7.0      1.0    155.0 -1.073320 -1.455156   \n995  4176.0  8.639500e+13     7.0      4.0     20.0 -1.019361 -1.177506   \n\n      Stat_14  Stat_15    Stat_16  Stat_17  Stat_18      Stat_19  \\\n0   -1.798073      0.0 -89.987045      0.0      0.0  3829.000000   \n1   -1.004992      0.0 -89.654587      0.0      0.0  3098.166748   \n2   -1.020313      0.0 -89.540176      0.0      0.0  3853.000000   \n3   -1.023798      0.0 -89.968369      0.0      0.0  3468.000000   \n4   -1.012266      0.0 -89.770241      0.0      0.0  3815.083252   \n..        ...      ...        ...      ...      ...          ...   \n991 -1.020204      0.0 -89.820068      0.0      0.0  3098.166748   \n992 -1.033333      0.0 -89.104843      0.0      0.0  3829.000000   \n993 -1.013423      0.0 -89.887924      0.0      0.0  3098.166748   \n994 -1.016536      0.0 -87.998444      0.0      0.0  4073.000000   \n995 -1.011560      0.0 -89.530304      0.0      0.0  3718.000000   \n\n          Stat_20  Stat_21  Stat_22  Stat_23   Stat_24   Stat_25   Stat_26  \\\n0    0.000000e+00      1.0      2.0     15.0 -0.054638 -0.163923 -0.114302   \n1    0.000000e+00      1.0      2.0    223.0  0.113277  0.093139 -0.106038   \n2    4.500000e+10      1.0      3.0     97.0 -0.499738  0.046381 -0.181152   \n3    0.000000e+00      1.0      1.0     48.0  0.007430  0.007583 -0.196510   \n4    3.500000e+10      1.0      4.0     20.0  0.086653 -0.115162 -0.138969   \n..            ...      ...      ...      ...       ...       ...       ...   \n991  0.000000e+00      1.0      4.0    138.0 -0.067798  0.006292  0.201008   \n992  4.325000e+12      1.0      1.0     75.0  0.097154 -0.356072 -0.043487   \n993  0.000000e+00      1.0      1.0     56.0 -0.147508 -0.047232 -0.242875   \n994  0.000000e+00      1.0      1.0    153.0 -0.441574 -0.080691 -0.270330   \n995  0.000000e+00      1.0      1.0      0.0 -0.181627 -0.300301  0.240738   \n\n      Stat_27    Stat_28   Stat_29    Stat_30      Stat_31       Stat_32  \\\n0    0.045252  -7.805897  0.000000  46.009533  4027.514893  5.415475e+13   \n1    0.028960  -6.065619  0.046508  56.437958  3829.466064  4.331149e+13   \n2    0.056544 -11.934993  0.000000  77.305130  4106.425781  4.481677e+13   \n3    0.053544 -12.847143  0.000000   9.369678  3958.604492  4.836642e+13   \n4    0.040399 -11.009835  0.000000   5.049157  3992.347656  5.833895e+13   \n..        ...        ...       ...        ...          ...           ...   \n991  0.038587  13.204196  0.005480   8.857800  3851.464844  4.327464e+13   \n992  0.036847  -6.769969  0.000000  35.192787  4002.345703  4.686980e+13   \n993  0.027135 -18.903458  0.222337  10.387013  3841.772705  4.316802e+13   \n994  0.037183 -17.535593  0.000000  11.325677  4123.798828  4.597792e+13   \n995  0.002993  14.728157  0.749290   5.465665  3891.058105  4.310022e+13   \n\n      Stat_33   Stat_34     Stat_35   Stat_36   Stat_37   Stat_38   Stat_39  \\\n0    4.438860  2.000000   30.202068 -0.701660 -0.619076 -0.536432  0.007953   \n1    3.840885  2.000000  232.909103 -0.231743 -0.257600 -0.595426  0.000367   \n2    3.148264  3.000000  100.144516 -0.873151 -0.255299 -0.485521  0.005643   \n3    4.273992  2.303057   60.025017 -0.530198 -0.412805 -0.556091  0.009947   \n4    4.541829  4.000000   46.192024 -0.224805 -0.444297 -0.685736  0.005364   \n..        ...       ...         ...       ...       ...       ...       ...   \n991  3.911399  4.000000  149.392853 -0.601601 -0.312998 -0.234954  0.001691   \n992  3.730544  1.548954  101.264435 -0.000378 -0.797474 -0.947604  0.008236   \n993  4.002807  1.000000   67.532288 -0.552659 -0.354082 -0.850300  0.000000   \n994  2.487963  1.000000  154.201294 -0.831641 -0.369779 -0.664401  0.009702   \n995  4.144179  2.860763   10.118834 -0.267668 -0.609891 -0.947463  0.000000   \n\n       Stat_40   Stat_41   Stat_42      Stat_43       Stat_44  Stat_45  \\\n0   -32.948602  0.000000  2.520257  3958.000000  4.325125e+13      3.0   \n1   -37.326844  0.000000  4.000000  3724.000000  2.128500e+13      2.0   \n2   -30.154542  0.000000  2.918126  4089.625000  2.888500e+13      3.0   \n3   -34.965618  0.000000  0.893617  3841.000000  3.526000e+13      3.0   \n4   -46.348264  0.000000  1.438378  3837.333252  5.161375e+13      3.0   \n..         ...       ...       ...          ...           ...      ...   \n991 -14.056123  0.000000  1.166667  3747.000000  2.141875e+13      2.0   \n992 -70.773838  0.000000  7.116813  3835.000000  4.487000e+13      3.0   \n993 -58.557291  0.000000  0.555556  3741.000000  2.137000e+13      2.0   \n994 -41.512409  0.000000  2.748235  4099.000000  2.505000e+13      1.0   \n995 -72.318169  0.488889  0.872747  3788.000000  2.139500e+13      3.0   \n\n     Stat_46  Stat_47   Stat_48   Stat_49   Stat_50   Stat_51    Stat_52  \\\n0        2.0     17.0  0.437897  0.148919  0.223770  0.036048  13.095750   \n1        2.0    228.0  0.517859  0.542323  0.312333  0.020598  18.462269   \n2        3.0     98.0 -0.242422  0.381953  0.088555  0.048282   5.009753   \n3        1.0     53.0  0.536801  0.443383  0.084469  0.057278   4.816339   \n4        4.0     32.0  0.544297  0.153125  0.347474  0.043690  20.726226   \n..       ...      ...       ...       ...       ...       ...        ...   \n991      4.0    144.0  0.438858  0.308362  0.662152  0.039577  42.374405   \n992      1.0     75.0  0.221654 -0.087578  0.632786  0.038638  38.104811   \n993      1.0     62.0  0.140716  0.280936  0.231454  0.012770  13.528161   \n994      1.0    154.0 -0.214362  0.210247  0.034375  0.039810   1.885406   \n995      1.0      5.0 -0.061189 -0.132583  0.863528  0.004280  57.963976   \n\n     Stat_53    Stat_54  Stat_55       Stat_56  Stat_57  Stat_58  Stat_59  \\\n0        0.0  24.750000   4146.0  6.978000e+13      6.0      2.0     38.0   \n1        0.0  27.490936   3958.0  6.511000e+13      5.0      2.0    238.0   \n2        0.0  21.022933   4140.0  6.094500e+13      4.0      3.0    100.0   \n3        0.0   6.200000   4064.0  6.330000e+13      6.0      4.0     67.0   \n4        0.0   4.942201   4087.0  7.393625e+13      7.0      4.0     69.0   \n..       ...        ...      ...           ...      ...      ...      ...   \n991      0.0   5.867146   3970.0  6.497625e+13      6.0      4.0    155.0   \n992      0.0  19.533333   4181.0  4.759750e+13      5.0      2.0    126.0   \n993      0.0   5.281850   3958.0  6.502500e+13      6.0      1.0     73.0   \n994      0.0  10.699164   4146.0  6.662500e+13      2.0      1.0    155.0   \n995      1.0   5.808605   3982.0  6.500000e+13      6.0      4.0     15.0   \n\n      Stat_60   Stat_61   Stat_62   Stat_63    Stat_64   Stat_65     Stat_66  \\\n0    0.633126  0.513286  0.500372  0.132576  34.917873  0.000000  205.862213   \n1    0.507897  0.541129  0.603787  0.096825  44.034721  0.208482  206.625092   \n2    0.454021  0.510668  0.412588  0.140594  27.367514  0.000000  274.848145   \n3    0.586100  0.542189  0.474437  0.103401  32.552841  0.000000   54.104408   \n4    0.509845  0.494897  0.639449  0.090201  47.933723  0.000000   15.590773   \n..        ...       ...       ...       ...        ...       ...         ...   \n991  0.591072  0.481551  0.565229  0.093812  38.950920  0.070666   53.159111   \n992  0.269882  0.530971  0.701824  0.068496  51.202812  0.000000   77.692108   \n993  0.478085  0.499994  0.622155  0.109624  48.017563  0.410910   75.709877   \n994  0.502446  0.457471  0.470241  0.064660  32.590225  0.000000   35.017689   \n995  0.260311  0.324098  0.799344  0.009499  60.556572  0.421324   29.646894   \n\n        Stat_67       Stat_68   Stat_69   Stat_70    Stat_71        id  \n0    108.451317  1.876976e+13  1.825557  0.000000  11.773107  0745c390  \n1    167.600983  2.509136e+13  1.957999  0.000000   5.701968  eaab7a96  \n2     50.734318  2.038156e+13  1.169176  0.000000   5.653936  8ec2cc63  \n3    122.706802  1.868773e+13  2.023705  1.487018   7.396456  b2987a65  \n4    126.121590  2.146206e+13  2.081796  0.000000  18.615358  7b8842c3  \n..          ...           ...       ...       ...        ...       ...  \n991  166.704880  2.502804e+13  1.951508  0.000000   6.595387  cd68643b  \n992  166.620529  7.143143e+12  1.230283  0.497806  25.009487  f8ff0bc8  \n993  164.142853  2.506494e+13  1.929882  0.000000   6.580971  db23fbe4  \n994   28.219002  2.436134e+13  2.188225  0.000000   0.726917  687c85e7  \n995  124.940826  2.503642e+13  1.964386  1.455972   5.731455  5f099188  \n\n[996 rows x 73 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Stat_0</th>\n      <th>Stat_1</th>\n      <th>Stat_2</th>\n      <th>Stat_3</th>\n      <th>Stat_4</th>\n      <th>Stat_5</th>\n      <th>Stat_6</th>\n      <th>Stat_7</th>\n      <th>Stat_8</th>\n      <th>Stat_9</th>\n      <th>Stat_10</th>\n      <th>Stat_11</th>\n      <th>Stat_12</th>\n      <th>Stat_13</th>\n      <th>Stat_14</th>\n      <th>Stat_15</th>\n      <th>Stat_16</th>\n      <th>Stat_17</th>\n      <th>Stat_18</th>\n      <th>Stat_19</th>\n      <th>Stat_20</th>\n      <th>Stat_21</th>\n      <th>Stat_22</th>\n      <th>Stat_23</th>\n      <th>Stat_24</th>\n      <th>Stat_25</th>\n      <th>Stat_26</th>\n      <th>Stat_27</th>\n      <th>Stat_28</th>\n      <th>Stat_29</th>\n      <th>Stat_30</th>\n      <th>Stat_31</th>\n      <th>Stat_32</th>\n      <th>Stat_33</th>\n      <th>Stat_34</th>\n      <th>Stat_35</th>\n      <th>Stat_36</th>\n      <th>Stat_37</th>\n      <th>Stat_38</th>\n      <th>Stat_39</th>\n      <th>Stat_40</th>\n      <th>Stat_41</th>\n      <th>Stat_42</th>\n      <th>Stat_43</th>\n      <th>Stat_44</th>\n      <th>Stat_45</th>\n      <th>Stat_46</th>\n      <th>Stat_47</th>\n      <th>Stat_48</th>\n      <th>Stat_49</th>\n      <th>Stat_50</th>\n      <th>Stat_51</th>\n      <th>Stat_52</th>\n      <th>Stat_53</th>\n      <th>Stat_54</th>\n      <th>Stat_55</th>\n      <th>Stat_56</th>\n      <th>Stat_57</th>\n      <th>Stat_58</th>\n      <th>Stat_59</th>\n      <th>Stat_60</th>\n      <th>Stat_61</th>\n      <th>Stat_62</th>\n      <th>Stat_63</th>\n      <th>Stat_64</th>\n      <th>Stat_65</th>\n      <th>Stat_66</th>\n      <th>Stat_67</th>\n      <th>Stat_68</th>\n      <th>Stat_69</th>\n      <th>Stat_70</th>\n      <th>Stat_71</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.850391</td>\n      <td>3.580182</td>\n      <td>1.738203</td>\n      <td>5.314874</td>\n      <td>89.422226</td>\n      <td>0.0</td>\n      <td>2626.199951</td>\n      <td>4187.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>57.0</td>\n      <td>-1.812031</td>\n      <td>-2.631380</td>\n      <td>-1.798073</td>\n      <td>0.0</td>\n      <td>-89.987045</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3829.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>15.0</td>\n      <td>-0.054638</td>\n      <td>-0.163923</td>\n      <td>-0.114302</td>\n      <td>0.045252</td>\n      <td>-7.805897</td>\n      <td>0.000000</td>\n      <td>46.009533</td>\n      <td>4027.514893</td>\n      <td>5.415475e+13</td>\n      <td>4.438860</td>\n      <td>2.000000</td>\n      <td>30.202068</td>\n      <td>-0.701660</td>\n      <td>-0.619076</td>\n      <td>-0.536432</td>\n      <td>0.007953</td>\n      <td>-32.948602</td>\n      <td>0.000000</td>\n      <td>2.520257</td>\n      <td>3958.000000</td>\n      <td>4.325125e+13</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>17.0</td>\n      <td>0.437897</td>\n      <td>0.148919</td>\n      <td>0.223770</td>\n      <td>0.036048</td>\n      <td>13.095750</td>\n      <td>0.0</td>\n      <td>24.750000</td>\n      <td>4146.0</td>\n      <td>6.978000e+13</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>38.0</td>\n      <td>0.633126</td>\n      <td>0.513286</td>\n      <td>0.500372</td>\n      <td>0.132576</td>\n      <td>34.917873</td>\n      <td>0.000000</td>\n      <td>205.862213</td>\n      <td>108.451317</td>\n      <td>1.876976e+13</td>\n      <td>1.825557</td>\n      <td>0.000000</td>\n      <td>11.773107</td>\n      <td>0745c390</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.928769</td>\n      <td>3.234613</td>\n      <td>2.475326</td>\n      <td>3.966906</td>\n      <td>89.080330</td>\n      <td>1.0</td>\n      <td>2628.199951</td>\n      <td>4146.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>243.0</td>\n      <td>-1.807955</td>\n      <td>-2.887664</td>\n      <td>-1.004992</td>\n      <td>0.0</td>\n      <td>-89.654587</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3098.166748</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>223.0</td>\n      <td>0.113277</td>\n      <td>0.093139</td>\n      <td>-0.106038</td>\n      <td>0.028960</td>\n      <td>-6.065619</td>\n      <td>0.046508</td>\n      <td>56.437958</td>\n      <td>3829.466064</td>\n      <td>4.331149e+13</td>\n      <td>3.840885</td>\n      <td>2.000000</td>\n      <td>232.909103</td>\n      <td>-0.231743</td>\n      <td>-0.257600</td>\n      <td>-0.595426</td>\n      <td>0.000367</td>\n      <td>-37.326844</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>3724.000000</td>\n      <td>2.128500e+13</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>228.0</td>\n      <td>0.517859</td>\n      <td>0.542323</td>\n      <td>0.312333</td>\n      <td>0.020598</td>\n      <td>18.462269</td>\n      <td>0.0</td>\n      <td>27.490936</td>\n      <td>3958.0</td>\n      <td>6.511000e+13</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>238.0</td>\n      <td>0.507897</td>\n      <td>0.541129</td>\n      <td>0.603787</td>\n      <td>0.096825</td>\n      <td>44.034721</td>\n      <td>0.208482</td>\n      <td>206.625092</td>\n      <td>167.600983</td>\n      <td>2.509136e+13</td>\n      <td>1.957999</td>\n      <td>0.000000</td>\n      <td>5.701968</td>\n      <td>eaab7a96</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.021510</td>\n      <td>1.016589</td>\n      <td>1.746797</td>\n      <td>5.066334</td>\n      <td>86.987267</td>\n      <td>0.0</td>\n      <td>2618.199951</td>\n      <td>4183.0</td>\n      <td>8.636500e+13</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>134.0</td>\n      <td>-1.903281</td>\n      <td>-3.150104</td>\n      <td>-1.020313</td>\n      <td>0.0</td>\n      <td>-89.540176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3853.000000</td>\n      <td>4.500000e+10</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>97.0</td>\n      <td>-0.499738</td>\n      <td>0.046381</td>\n      <td>-0.181152</td>\n      <td>0.056544</td>\n      <td>-11.934993</td>\n      <td>0.000000</td>\n      <td>77.305130</td>\n      <td>4106.425781</td>\n      <td>4.481677e+13</td>\n      <td>3.148264</td>\n      <td>3.000000</td>\n      <td>100.144516</td>\n      <td>-0.873151</td>\n      <td>-0.255299</td>\n      <td>-0.485521</td>\n      <td>0.005643</td>\n      <td>-30.154542</td>\n      <td>0.000000</td>\n      <td>2.918126</td>\n      <td>4089.625000</td>\n      <td>2.888500e+13</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>98.0</td>\n      <td>-0.242422</td>\n      <td>0.381953</td>\n      <td>0.088555</td>\n      <td>0.048282</td>\n      <td>5.009753</td>\n      <td>0.0</td>\n      <td>21.022933</td>\n      <td>4140.0</td>\n      <td>6.094500e+13</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>100.0</td>\n      <td>0.454021</td>\n      <td>0.510668</td>\n      <td>0.412588</td>\n      <td>0.140594</td>\n      <td>27.367514</td>\n      <td>0.000000</td>\n      <td>274.848145</td>\n      <td>50.734318</td>\n      <td>2.038156e+13</td>\n      <td>1.169176</td>\n      <td>0.000000</td>\n      <td>5.653936</td>\n      <td>8ec2cc63</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.908000</td>\n      <td>2.083693</td>\n      <td>1.269051</td>\n      <td>6.134459</td>\n      <td>89.976074</td>\n      <td>0.0</td>\n      <td>2502.000000</td>\n      <td>6000.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>72.0</td>\n      <td>-1.684624</td>\n      <td>-2.405738</td>\n      <td>-1.023798</td>\n      <td>0.0</td>\n      <td>-89.968369</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3468.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>48.0</td>\n      <td>0.007430</td>\n      <td>0.007583</td>\n      <td>-0.196510</td>\n      <td>0.053544</td>\n      <td>-12.847143</td>\n      <td>0.000000</td>\n      <td>9.369678</td>\n      <td>3958.604492</td>\n      <td>4.836642e+13</td>\n      <td>4.273992</td>\n      <td>2.303057</td>\n      <td>60.025017</td>\n      <td>-0.530198</td>\n      <td>-0.412805</td>\n      <td>-0.556091</td>\n      <td>0.009947</td>\n      <td>-34.965618</td>\n      <td>0.000000</td>\n      <td>0.893617</td>\n      <td>3841.000000</td>\n      <td>3.526000e+13</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>53.0</td>\n      <td>0.536801</td>\n      <td>0.443383</td>\n      <td>0.084469</td>\n      <td>0.057278</td>\n      <td>4.816339</td>\n      <td>0.0</td>\n      <td>6.200000</td>\n      <td>4064.0</td>\n      <td>6.330000e+13</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>67.0</td>\n      <td>0.586100</td>\n      <td>0.542189</td>\n      <td>0.474437</td>\n      <td>0.103401</td>\n      <td>32.552841</td>\n      <td>0.000000</td>\n      <td>54.104408</td>\n      <td>122.706802</td>\n      <td>1.868773e+13</td>\n      <td>2.023705</td>\n      <td>1.487018</td>\n      <td>7.396456</td>\n      <td>b2987a65</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.231563</td>\n      <td>1.033620</td>\n      <td>1.071875</td>\n      <td>2.774382</td>\n      <td>89.300034</td>\n      <td>0.0</td>\n      <td>1046.800049</td>\n      <td>4199.0</td>\n      <td>8.601500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>76.0</td>\n      <td>-1.675859</td>\n      <td>-1.071042</td>\n      <td>-1.012266</td>\n      <td>0.0</td>\n      <td>-89.770241</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3815.083252</td>\n      <td>3.500000e+10</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>20.0</td>\n      <td>0.086653</td>\n      <td>-0.115162</td>\n      <td>-0.138969</td>\n      <td>0.040399</td>\n      <td>-11.009835</td>\n      <td>0.000000</td>\n      <td>5.049157</td>\n      <td>3992.347656</td>\n      <td>5.833895e+13</td>\n      <td>4.541829</td>\n      <td>4.000000</td>\n      <td>46.192024</td>\n      <td>-0.224805</td>\n      <td>-0.444297</td>\n      <td>-0.685736</td>\n      <td>0.005364</td>\n      <td>-46.348264</td>\n      <td>0.000000</td>\n      <td>1.438378</td>\n      <td>3837.333252</td>\n      <td>5.161375e+13</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>32.0</td>\n      <td>0.544297</td>\n      <td>0.153125</td>\n      <td>0.347474</td>\n      <td>0.043690</td>\n      <td>20.726226</td>\n      <td>0.0</td>\n      <td>4.942201</td>\n      <td>4087.0</td>\n      <td>7.393625e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>69.0</td>\n      <td>0.509845</td>\n      <td>0.494897</td>\n      <td>0.639449</td>\n      <td>0.090201</td>\n      <td>47.933723</td>\n      <td>0.000000</td>\n      <td>15.590773</td>\n      <td>126.121590</td>\n      <td>2.146206e+13</td>\n      <td>2.081796</td>\n      <td>0.000000</td>\n      <td>18.615358</td>\n      <td>7b8842c3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>991</th>\n      <td>1.491908</td>\n      <td>3.059684</td>\n      <td>2.099614</td>\n      <td>3.669502</td>\n      <td>89.025551</td>\n      <td>1.0</td>\n      <td>2576.399902</td>\n      <td>4191.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>161.0</td>\n      <td>-1.407426</td>\n      <td>-1.014350</td>\n      <td>-1.020204</td>\n      <td>0.0</td>\n      <td>-89.820068</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3098.166748</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>138.0</td>\n      <td>-0.067798</td>\n      <td>0.006292</td>\n      <td>0.201008</td>\n      <td>0.038587</td>\n      <td>13.204196</td>\n      <td>0.005480</td>\n      <td>8.857800</td>\n      <td>3851.464844</td>\n      <td>4.327464e+13</td>\n      <td>3.911399</td>\n      <td>4.000000</td>\n      <td>149.392853</td>\n      <td>-0.601601</td>\n      <td>-0.312998</td>\n      <td>-0.234954</td>\n      <td>0.001691</td>\n      <td>-14.056123</td>\n      <td>0.000000</td>\n      <td>1.166667</td>\n      <td>3747.000000</td>\n      <td>2.141875e+13</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>144.0</td>\n      <td>0.438858</td>\n      <td>0.308362</td>\n      <td>0.662152</td>\n      <td>0.039577</td>\n      <td>42.374405</td>\n      <td>0.0</td>\n      <td>5.867146</td>\n      <td>3970.0</td>\n      <td>6.497625e+13</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>155.0</td>\n      <td>0.591072</td>\n      <td>0.481551</td>\n      <td>0.565229</td>\n      <td>0.093812</td>\n      <td>38.950920</td>\n      <td>0.070666</td>\n      <td>53.159111</td>\n      <td>166.704880</td>\n      <td>2.502804e+13</td>\n      <td>1.951508</td>\n      <td>0.000000</td>\n      <td>6.595387</td>\n      <td>cd68643b</td>\n    </tr>\n    <tr>\n      <th>992</th>\n      <td>1.353594</td>\n      <td>0.994583</td>\n      <td>0.996484</td>\n      <td>1.786410</td>\n      <td>81.665283</td>\n      <td>0.0</td>\n      <td>1526.599976</td>\n      <td>4194.0</td>\n      <td>8.514000e+13</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>130.0</td>\n      <td>-1.064844</td>\n      <td>-1.012995</td>\n      <td>-1.033333</td>\n      <td>0.0</td>\n      <td>-89.104843</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3829.000000</td>\n      <td>4.325000e+12</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>75.0</td>\n      <td>0.097154</td>\n      <td>-0.356072</td>\n      <td>-0.043487</td>\n      <td>0.036847</td>\n      <td>-6.769969</td>\n      <td>0.000000</td>\n      <td>35.192787</td>\n      <td>4002.345703</td>\n      <td>4.686980e+13</td>\n      <td>3.730544</td>\n      <td>1.548954</td>\n      <td>101.264435</td>\n      <td>-0.000378</td>\n      <td>-0.797474</td>\n      <td>-0.947604</td>\n      <td>0.008236</td>\n      <td>-70.773838</td>\n      <td>0.000000</td>\n      <td>7.116813</td>\n      <td>3835.000000</td>\n      <td>4.487000e+13</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>75.0</td>\n      <td>0.221654</td>\n      <td>-0.087578</td>\n      <td>0.632786</td>\n      <td>0.038638</td>\n      <td>38.104811</td>\n      <td>0.0</td>\n      <td>19.533333</td>\n      <td>4181.0</td>\n      <td>4.759750e+13</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>126.0</td>\n      <td>0.269882</td>\n      <td>0.530971</td>\n      <td>0.701824</td>\n      <td>0.068496</td>\n      <td>51.202812</td>\n      <td>0.000000</td>\n      <td>77.692108</td>\n      <td>166.620529</td>\n      <td>7.143143e+12</td>\n      <td>1.230283</td>\n      <td>0.497806</td>\n      <td>25.009487</td>\n      <td>f8ff0bc8</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>0.999923</td>\n      <td>1.043029</td>\n      <td>1.547813</td>\n      <td>3.692727</td>\n      <td>89.333710</td>\n      <td>1.0</td>\n      <td>2592.199951</td>\n      <td>4178.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>79.0</td>\n      <td>-1.508058</td>\n      <td>-2.958281</td>\n      <td>-1.013423</td>\n      <td>0.0</td>\n      <td>-89.887924</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3098.166748</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>56.0</td>\n      <td>-0.147508</td>\n      <td>-0.047232</td>\n      <td>-0.242875</td>\n      <td>0.027135</td>\n      <td>-18.903458</td>\n      <td>0.222337</td>\n      <td>10.387013</td>\n      <td>3841.772705</td>\n      <td>4.316802e+13</td>\n      <td>4.002807</td>\n      <td>1.000000</td>\n      <td>67.532288</td>\n      <td>-0.552659</td>\n      <td>-0.354082</td>\n      <td>-0.850300</td>\n      <td>0.000000</td>\n      <td>-58.557291</td>\n      <td>0.000000</td>\n      <td>0.555556</td>\n      <td>3741.000000</td>\n      <td>2.137000e+13</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>62.0</td>\n      <td>0.140716</td>\n      <td>0.280936</td>\n      <td>0.231454</td>\n      <td>0.012770</td>\n      <td>13.528161</td>\n      <td>0.0</td>\n      <td>5.281850</td>\n      <td>3958.0</td>\n      <td>6.502500e+13</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>73.0</td>\n      <td>0.478085</td>\n      <td>0.499994</td>\n      <td>0.622155</td>\n      <td>0.109624</td>\n      <td>48.017563</td>\n      <td>0.410910</td>\n      <td>75.709877</td>\n      <td>164.142853</td>\n      <td>2.506494e+13</td>\n      <td>1.929882</td>\n      <td>0.000000</td>\n      <td>6.580971</td>\n      <td>db23fbe4</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>1.004674</td>\n      <td>0.981576</td>\n      <td>0.999219</td>\n      <td>1.673958</td>\n      <td>88.629547</td>\n      <td>0.0</td>\n      <td>1875.199951</td>\n      <td>4183.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>155.0</td>\n      <td>-1.073320</td>\n      <td>-1.455156</td>\n      <td>-1.016536</td>\n      <td>0.0</td>\n      <td>-87.998444</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4073.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>153.0</td>\n      <td>-0.441574</td>\n      <td>-0.080691</td>\n      <td>-0.270330</td>\n      <td>0.037183</td>\n      <td>-17.535593</td>\n      <td>0.000000</td>\n      <td>11.325677</td>\n      <td>4123.798828</td>\n      <td>4.597792e+13</td>\n      <td>2.487963</td>\n      <td>1.000000</td>\n      <td>154.201294</td>\n      <td>-0.831641</td>\n      <td>-0.369779</td>\n      <td>-0.664401</td>\n      <td>0.009702</td>\n      <td>-41.512409</td>\n      <td>0.000000</td>\n      <td>2.748235</td>\n      <td>4099.000000</td>\n      <td>2.505000e+13</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>154.0</td>\n      <td>-0.214362</td>\n      <td>0.210247</td>\n      <td>0.034375</td>\n      <td>0.039810</td>\n      <td>1.885406</td>\n      <td>0.0</td>\n      <td>10.699164</td>\n      <td>4146.0</td>\n      <td>6.662500e+13</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>155.0</td>\n      <td>0.502446</td>\n      <td>0.457471</td>\n      <td>0.470241</td>\n      <td>0.064660</td>\n      <td>32.590225</td>\n      <td>0.000000</td>\n      <td>35.017689</td>\n      <td>28.219002</td>\n      <td>2.436134e+13</td>\n      <td>2.188225</td>\n      <td>0.000000</td>\n      <td>0.726917</td>\n      <td>687c85e7</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>1.015231</td>\n      <td>1.051578</td>\n      <td>1.006835</td>\n      <td>1.009104</td>\n      <td>88.652969</td>\n      <td>1.0</td>\n      <td>1196.599976</td>\n      <td>4176.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>20.0</td>\n      <td>-1.019361</td>\n      <td>-1.177506</td>\n      <td>-1.011560</td>\n      <td>0.0</td>\n      <td>-89.530304</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3718.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-0.181627</td>\n      <td>-0.300301</td>\n      <td>0.240738</td>\n      <td>0.002993</td>\n      <td>14.728157</td>\n      <td>0.749290</td>\n      <td>5.465665</td>\n      <td>3891.058105</td>\n      <td>4.310022e+13</td>\n      <td>4.144179</td>\n      <td>2.860763</td>\n      <td>10.118834</td>\n      <td>-0.267668</td>\n      <td>-0.609891</td>\n      <td>-0.947463</td>\n      <td>0.000000</td>\n      <td>-72.318169</td>\n      <td>0.488889</td>\n      <td>0.872747</td>\n      <td>3788.000000</td>\n      <td>2.139500e+13</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>-0.061189</td>\n      <td>-0.132583</td>\n      <td>0.863528</td>\n      <td>0.004280</td>\n      <td>57.963976</td>\n      <td>1.0</td>\n      <td>5.808605</td>\n      <td>3982.0</td>\n      <td>6.500000e+13</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>0.260311</td>\n      <td>0.324098</td>\n      <td>0.799344</td>\n      <td>0.009499</td>\n      <td>60.556572</td>\n      <td>0.421324</td>\n      <td>29.646894</td>\n      <td>124.940826</td>\n      <td>2.503642e+13</td>\n      <td>1.964386</td>\n      <td>1.455972</td>\n      <td>5.731455</td>\n      <td>5f099188</td>\n    </tr>\n  </tbody>\n</table>\n<p>996 rows × 73 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# transform \nscale_train_ts = train_ts.drop('id', axis=1)\nscale_train_ts_cols = scale_train_ts.columns\nscaler = MinMaxScaler()\nscaler.fit(scale_train_ts)\nscale_train_ts = pd.DataFrame(scaler.transform(scale_train_ts), columns = scale_train_ts_cols)\nscale_train_ts['id'] = train_ts['id']\n\n\nscale_test_ts = test_ts.drop('id', axis=1)\nscale_test_ts_cols = scale_test_ts.columns\nscale_test_ts = pd.DataFrame(scaler.transform(scale_test_ts), columns = scale_test_ts_cols)\nscale_test_ts['id'] = test_ts['id']\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:33:46.720813Z","iopub.execute_input":"2024-12-24T10:33:46.721155Z","iopub.status.idle":"2024-12-24T10:33:46.735936Z","shell.execute_reply.started":"2024-12-24T10:33:46.721124Z","shell.execute_reply":"2024-12-24T10:33:46.735068Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# List columns of time series\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\n# add new features for csv\n# train = create_feature_csv(train)\n# test = create_feature_csv(test)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:40:07.060735Z","iopub.execute_input":"2024-12-24T10:40:07.061769Z","iopub.status.idle":"2024-12-24T10:40:07.067211Z","shell.execute_reply.started":"2024-12-24T10:40:07.061715Z","shell.execute_reply":"2024-12-24T10:40:07.065956Z"},"papermill":{"duration":0.042766,"end_time":"2024-12-19T21:31:27.387579","exception":false,"start_time":"2024-12-19T21:31:27.344813","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# merge csv and time series\ntrain = pd.merge(train, scale_train_ts, how=\"left\", on='id')\ntest = pd.merge(test, scale_test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:33:46.745476Z","iopub.execute_input":"2024-12-24T10:33:46.745773Z","iopub.status.idle":"2024-12-24T10:33:46.780690Z","shell.execute_reply.started":"2024-12-24T10:33:46.745744Z","shell.execute_reply":"2024-12-24T10:33:46.779798Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# choose features\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii'] + time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ntest = test[[c for c in featuresCols if c !=\"sii\"]]","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:33:46.783162Z","iopub.execute_input":"2024-12-24T10:33:46.783507Z","iopub.status.idle":"2024-12-24T10:33:46.795975Z","shell.execute_reply.started":"2024-12-24T10:33:46.783458Z","shell.execute_reply":"2024-12-24T10:33:46.795064Z"},"papermill":{"duration":0.123355,"end_time":"2024-12-19T21:31:47.851076","exception":false,"start_time":"2024-12-19T21:31:47.727721","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# categorical encoding \nnot_numeric_data = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef categorical_encoding(df, columns, ref_df=None):\n    for d in columns:\n        df[d] = df[d].fillna('Missing') # Fill in NA values\n        if ref_df is not None:\n            # Using mapping from ref_df set ensures data consistency\n            categories = ref_df[d].astype('category').cat.categories\n            df[d] = pd.Categorical(df[d], categories=categories).codes\n        else:\n            df[d] = df[d].astype('category').cat.codes\n    return df\n\ntrain = categorical_encoding(train, not_numeric_data)\ntest = categorical_encoding(test, not_numeric_data, ref_df=train)\n        ","metadata":{"papermill":{"duration":0.029239,"end_time":"2024-12-19T21:31:47.910047","exception":false,"start_time":"2024-12-19T21:31:47.880808","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:33:46.797035Z","iopub.execute_input":"2024-12-24T10:33:46.797457Z","iopub.status.idle":"2024-12-24T10:33:46.835709Z","shell.execute_reply.started":"2024-12-24T10:33:46.797411Z","shell.execute_reply":"2024-12-24T10:33:46.834874Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(non_rounded_pred, thresholds):\n    return np.where(non_rounded_pred < thresholds[0], 0,\n           np.where(non_rounded_pred < thresholds[1], 1,\n           np.where(non_rounded_pred < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, non_rounded_pred):\n    rounded_p = threshold_Rounder(non_rounded_pred, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:33:46.837163Z","iopub.execute_input":"2024-12-24T10:33:46.837544Z","iopub.status.idle":"2024-12-24T10:33:46.844389Z","shell.execute_reply.started":"2024-12-24T10:33:46.837504Z","shell.execute_reply":"2024-12-24T10:33:46.843260Z"},"papermill":{"duration":0.037466,"end_time":"2024-12-19T21:31:47.977187","exception":false,"start_time":"2024-12-19T21:31:47.939721","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_and_evaluate_model(model_class, test_data):\n    \n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    # SKF = RepeatedStratifiedKFold (n_splits=n_splits, n_repeats = 3, random_state=SEED)\n    \n    train_result = []\n    test_result = []\n    \n    non_rounded_pred = np.zeros(len(y), dtype=float) \n    rounded_pred = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n      \n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        # predict\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n       \n        non_rounded_pred[test_idx] = y_val_pred\n        \n       \n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n\n        \n        rounded_pred[test_idx] = y_val_pred_rounded\n\n       \n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_result.append(train_kappa)\n        test_result.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n\n    print(f\"Mean Train QWK --> {np.mean(train_result):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_result):.4f}\")\n\n   \n    KappaOptimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, non_rounded_pred), \n                              method='Nelder-Mead') \n  \n\n    predicted_tuned = threshold_Rounder(non_rounded_pred, KappaOptimizer.x)\n    tKappa = quadratic_weighted_kappa(y, predicted_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOptimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission,model,tKappa","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:33:46.845797Z","iopub.execute_input":"2024-12-24T10:33:46.846318Z","iopub.status.idle":"2024-12-24T10:33:46.858249Z","shell.execute_reply.started":"2024-12-24T10:33:46.846274Z","shell.execute_reply":"2024-12-24T10:33:46.857272Z"},"papermill":{"duration":0.040514,"end_time":"2024-12-19T21:31:48.047121","exception":false,"start_time":"2024-12-19T21:31:48.006607","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# def objective(trial):\n#     XGBoostParams = {\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n#         'max_depth': trial.suggest_int('max_depth', 6, 8),\n#         'n_estimators': trial.suggest_int('n_estimators', 180, 500),\n#         'subsample': trial.suggest_uniform('subsample', 0.7, 0.9),\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 0.9),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1, 7),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 3),\n#         'tree_method': 'gpu_hist',\n#         'random_state': SEED\n#     }\n\n#     # CatBoostParams = {\n#     #    'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),  \n#     #    'depth': trial.suggest_int('depth', 6, 8),  \n#     #    'iterations': trial.suggest_int('iterations', 200, 400),  \n#     #    'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 2, 7),  \n#     #    'border_count': 128,  \n#     #    'random_seed': SEED, \n#     #    'task_type': 'GPU', \n#     #    'verbose': 0  \n#     # }\n\n#     # LgbmParams = {\n#     #    'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n#     #    'max_depth': trial.suggest_int('max_depth', 10, 14),\n#     #    'num_leaves': 60,\n#     #    'min_data_in_leaf': 14,\n#     #    'feature_fraction': trial.suggest_uniform('feature_fraction', 0.7, 0.9),\n#     #    'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.7, 0.9),\n#     #    'bagging_freq': 2,\n#     #    'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-5, 3),\n#     #    'lambda_l2': trial.suggest_loguniform('lambda_l2', 1, 6),\n#     #    'random_state': 42,\n#     #    'verbose': -1,\n#     #    'n_estimators': trial.suggest_int('n_estimators', 200, 500)  \n#     # }\n    \n#     # model_class = CatBoostRegressor(**CatBoostParams)\n#     # model_class = lgb.LGBMRegressor(**LgbmParams)\n#     model_class = XGBRegressor(**XGBoostParams)\n#     submission, model, tKappa = train_and_evaluate_model(model_class, test)\n#     return tKappa\n    \n\n\n# sampler = TPESampler(seed=SEED) # perform parameter selection based on conditional probability\n# # If the current result does not exceed the average value of previous tests => stop early\n# pruner = MedianPruner(n_warmup_steps=10) \n# study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n# study.optimize(objective, n_trials=60)\n\n# print(\"Best trial:\")\n# trial = study.best_trial\n# print(trial.params)\n# print(f\"Best QWK Score: {trial.value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:33:46.859270Z","iopub.execute_input":"2024-12-24T10:33:46.859599Z","iopub.status.idle":"2024-12-24T10:33:46.874548Z","shell.execute_reply.started":"2024-12-24T10:33:46.859519Z","shell.execute_reply":"2024-12-24T10:33:46.873623Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Params = {\n    'learning_rate': 0.04213252223,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  \n    'lambda_l2': 0.01  \n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 234,\n    'subsample': 0.7912213,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  \n    'reg_lambda': 5,  \n    'random_state': SEED\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 212,\n    'random_seed': SEED,\n    'cat_features': not_numeric_data,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\n# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T10:33:46.875813Z","iopub.execute_input":"2024-12-24T10:33:46.876178Z","iopub.status.idle":"2024-12-24T10:33:46.896085Z","shell.execute_reply.started":"2024-12-24T10:33:46.876147Z","shell.execute_reply":"2024-12-24T10:33:46.894832Z"},"papermill":{"duration":15.038502,"end_time":"2024-12-19T21:32:03.25006","exception":false,"start_time":"2024-12-19T21:31:48.211558","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# # fine tune Voting Regressor\n# from scipy.optimize import minimize \n# def optimize_weights(weights):\n\n#     print(f\"Current Weights: {weights}\")\n#     ensemble = VotingRegressor(estimators=[\n#         ('lightgbm', Light),\n#         ('xgboost', XGB_Model),\n#         ('catboost', CatBoost_Model)\n#     ], weights=weights)\n\n#     submission, model, tkappa = TrainML(ensemble, test)\n#     return -tkappa  \n\n# # Initial initial weight\n# initial_weights = [1.0, 1.0, 1.0]\n\n# result = minimize(\n#     optimize_weights, \n#     initial_weights, \n#     method='Nelder-Mead', \n#     options={'disp': True, 'maxiter': 100} \n# )\n\n# best_weights = result.x\n# print(\"Best Weights:\", best_weights)\nbest_weights = [4.9,5.2,4.9]","metadata":{"papermill":{"duration":0.03107,"end_time":"2024-12-19T21:32:03.316423","exception":false,"start_time":"2024-12-19T21:32:03.285353","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:38:14.074008Z","iopub.execute_input":"2024-12-24T10:38:14.074379Z","iopub.status.idle":"2024-12-24T10:38:14.079897Z","shell.execute_reply.started":"2024-12-24T10:38:14.074349Z","shell.execute_reply":"2024-12-24T10:38:14.078723Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n],weights=best_weights)\n\n\nsubmission, model, tkappa = train_and_evaluate_model(voting_model, test)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:38:16.289858Z","iopub.execute_input":"2024-12-24T10:38:16.291088Z","iopub.status.idle":"2024-12-24T10:39:33.990403Z","shell.execute_reply.started":"2024-12-24T10:38:16.291046Z","shell.execute_reply":"2024-12-24T10:39:33.989410Z"}},"outputs":[{"name":"stderr","text":"Training Folds:  20%|██        | 1/5 [00:15<01:03, 15.90s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Train QWK: 0.7659, Validation QWK: 0.3730\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  40%|████      | 2/5 [00:31<00:47, 15.77s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 2 - Train QWK: 0.7677, Validation QWK: 0.4313\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  60%|██████    | 3/5 [00:46<00:30, 15.50s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Train QWK: 0.7782, Validation QWK: 0.4110\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  80%|████████  | 4/5 [01:02<00:15, 15.66s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Train QWK: 0.7802, Validation QWK: 0.3602\n","output_type":"stream"},{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [01:17<00:00, 15.48s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 5 - Train QWK: 0.7724, Validation QWK: 0.3967\nMean Train QWK --> 0.7729\nMean Validation QWK ---> 0.3945\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"----> || Optimized QWK SCORE :: 0.457\n","output_type":"stream"}],"execution_count":21}]}